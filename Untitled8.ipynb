{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKs2X_WghtLY",
        "outputId": "51e630d0-17b8-4eb3-c61b-741cd5a848a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emails trouvés:\n",
            "contact@darlowparis.com\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "text = \"\"\n",
        "# Fonction pour extraire les e-mails d'une page web\n",
        "def extract_emails_from_url(url):\n",
        "    # Envoyer une requête GET à l'URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Vérifier que la requête a été réussie\n",
        "    if response.status_code == 200:\n",
        "        # Utiliser BeautifulSoup pour parser le contenu HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Convertir le texte de la page en string\n",
        "        text =soup.get_text()\n",
        "\n",
        "        # Utiliser une expression régulière pour trouver toutes les adresses e-mail\n",
        "        emails = re.findall(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}', text)\n",
        "\n",
        "        return emails\n",
        "    else:\n",
        "        print(f\"Erreur: impossible d'accéder à {url}\")\n",
        "        return []\n",
        "\n",
        "# Exemple d'utilisation\n",
        "url = 'https://www.google.com/search?q=\"@darlowparis.com\"'  # Remplacez par l'URL de votre choix\n",
        "emails = extract_emails_from_url(url)\n",
        "\n",
        "if emails:\n",
        "    print(\"Emails trouvés:\")\n",
        "    for email in emails:\n",
        "        print(email)\n",
        "else:\n",
        "    print(\"Aucun email trouvé.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Fonction pour extraire les e-mails d'une page web\n",
        "def extract_emails_from_url(url):\n",
        "\n",
        "    try:\n",
        "        url1 ='https://www.google.com/search?q=\"'+url+'\"'\n",
        "        # Envoyer une requête GET à l'URL\n",
        "        response = requests.get(url1)\n",
        "        response.raise_for_status()  # Vérifie si la requête a réussi\n",
        "\n",
        "        # Utiliser BeautifulSoup pour parser le contenu HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Convertir le texte de la page en string\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Utiliser une expression régulière pour trouver toutes les adresses e-mail\n",
        "        emails = re.findall(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}', text)\n",
        "\n",
        "        return emails\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erreur lors de l'accès à {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Liste des URLs à scraper\n",
        "urls = [\n",
        "    \"darlowparis.com\",\n",
        "    \"darlowparis.com\"\n",
        "    # Ajoutez autant d'URLs que nécessaire\n",
        "]\n",
        "\n",
        "# Dictionnaire pour stocker les résultats\n",
        "emails_dict = {}\n",
        "\n",
        "# Extraire les e-mails de chaque site web dans la liste\n",
        "for url in urls:\n",
        "    print(f\"Scraping {url}...\")\n",
        "    emails = extract_emails_from_url(url)\n",
        "    if emails:\n",
        "        emails_dict[url] = emails\n",
        "        print(f\"Emails trouvés sur {url}: {emails}\")\n",
        "    else:\n",
        "        print(f\"Aucun email trouvé sur {url}\")\n",
        "\n",
        "# Optionnel : Sauvegarder les résultats dans un fichier\n",
        "with open('emails_extracted.txt', 'w') as file:\n",
        "    for url, emails in emails_dict.items():\n",
        "        file.write(f\"Site: {url}\\n\")\n",
        "        for email in emails:\n",
        "            file.write(f\"{email}\\n\")\n",
        "        file.write(\"\\n\")\n",
        "\n",
        "print(\"Extraction terminée. Résultats sauvegardés dans 'emails_extracted.txt'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QumPiRVJoq_g",
        "outputId": "cb12eb71-a62b-419c-8905-024c23c4fad0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping darlowparis.com...\n",
            "Emails trouvés sur darlowparis.com: ['contact@darlowparis.com']\n",
            "Scraping darlowparis.com...\n",
            "Aucun email trouvé sur darlowparis.com\n",
            "Extraction terminée. Résultats sauvegardés dans 'emails_extracted.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Path to your local HTML file\n",
        "file_path = 'path/to/your/file.html'  # Replace with your file path\n",
        "\n",
        "# Read the content of the HTML file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "# Define the specific class you're looking for in the <div>\n",
        "divs = soup.find_all('div', class_='Nv2PK tH5CWc THOPZb')\n",
        "\n",
        "# Extract links from the <a> tags within the specified divs\n",
        "links = []\n",
        "for div in divs:\n",
        "    a_tag = div.find('a', class_='hfpxzc')\n",
        "    if a_tag and 'href' in a_tag.attrs:\n",
        "        links.append(a_tag['href'])\n",
        "\n",
        "# Print the extracted links\n",
        "for link in links:\n",
        "    print(link)\n"
      ],
      "metadata": {
        "id": "ti6TbUAFiftz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnesujP4i901",
        "outputId": "b01c46a5-4e25-459f-8f8e-cf6e2a6976c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}